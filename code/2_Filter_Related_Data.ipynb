{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abdd822b-a960-4b4c-9817-7962373991c8",
   "metadata": {},
   "source": [
    "### Filter related TSV files based on English Community Notes\n",
    "### This script processes other TSV files to keep only data related to English notes\n",
    "### Fixed the issue with different user ID column names across different files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cbf1478-3867-4512-af55-8305e093cd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03da3cb3-d50d-4999-906e-0b4d97272a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "english_notes_path = \"../data/english_only/english_notes-00000.tsv\"\n",
    "english_note_ids_path = \"../data/english_only/english_note_ids.npy\"\n",
    "\n",
    "# Input files\n",
    "ratings_path = \"../data/raw/ratings-00003.tsv\"\n",
    "note_status_history_path = \"../data/raw/noteStatusHistory-00000.tsv\"\n",
    "user_enrollment_path = \"../data/raw/userEnrollment-00000.tsv\"\n",
    "\n",
    "# Output files\n",
    "output_dir = \"../data/english_only/\"\n",
    "output_ratings_path = os.path.join(output_dir, \"english_ratings-00003.tsv\")\n",
    "output_note_status_history_path = os.path.join(output_dir, \"english_noteStatusHistory-00000.tsv\")\n",
    "output_user_enrollment_path = os.path.join(output_dir, \"english_userEnrollment-00000.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85c190b2-61c1-4386-be6b-3e21afe7cd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f60646df-b02c-4608-9c7d-f5b246c6e818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading English note IDs...\n",
      "Loaded 1115550 English note IDs\n",
      "Getting authors of English notes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r1/2mcrr6nx5hg7d13ln1lw2l5w0000gn/T/ipykernel_14742/1138722892.py:18: DtypeWarning: Columns (5,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  english_notes_df = pd.read_csv(english_notes_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found user ID column in notes file: noteAuthorParticipantId\n",
      "Found 165997 unique authors of English notes\n"
     ]
    }
   ],
   "source": [
    "# Load English note IDs\n",
    "print(\"Loading English note IDs...\")\n",
    "if os.path.exists(english_note_ids_path):\n",
    "    english_note_ids = np.load(english_note_ids_path, allow_pickle=True)\n",
    "    print(f\"Loaded {len(english_note_ids)} English note IDs\")\n",
    "else:\n",
    "    # If the IDs file doesn't exist, read the English notes file and extract IDs\n",
    "    print(\"English note IDs file not found. Reading from English notes file...\")\n",
    "    english_notes_df = pd.read_csv(english_notes_path, sep='\\t')\n",
    "    english_note_ids = english_notes_df['noteId'].tolist()\n",
    "    print(f\"Extracted {len(english_note_ids)} English note IDs\")\n",
    "\n",
    "# Convert to set for faster lookups\n",
    "english_note_ids_set = set(english_note_ids)\n",
    "\n",
    "# Get authors of English notes\n",
    "print(\"Getting authors of English notes...\")\n",
    "english_notes_df = pd.read_csv(english_notes_path, sep='\\t')\n",
    "# Check user ID column name in notes file\n",
    "notes_user_id_column = None\n",
    "notes_possible_id_columns = ['participantId', 'noteAuthorParticipantId']\n",
    "for col in notes_possible_id_columns:\n",
    "    if col in english_notes_df.columns:\n",
    "        notes_user_id_column = col\n",
    "        print(f\"Found user ID column in notes file: {notes_user_id_column}\")\n",
    "        break\n",
    "\n",
    "if notes_user_id_column:\n",
    "    english_authors = set(english_notes_df[notes_user_id_column].unique())\n",
    "    print(f\"Found {len(english_authors)} unique authors of English notes\")\n",
    "else:\n",
    "    print(\"Warning: Could not find user ID column in notes file\")\n",
    "    english_authors = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd530de4-2003-4c82-8f07-e232e9595944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ratings file: ../data/raw/ratings-00003.tsv\n",
      "Ratings file columns: ['noteId', 'raterParticipantId', 'createdAtMillis', 'version', 'agree', 'disagree', 'helpful', 'notHelpful', 'helpfulnessLevel', 'helpfulOther', 'helpfulInformative', 'helpfulClear', 'helpfulEmpathetic', 'helpfulGoodSources', 'helpfulUniqueContext', 'helpfulAddressesClaim', 'helpfulImportantContext', 'helpfulUnbiasedLanguage', 'notHelpfulOther', 'notHelpfulIncorrect', 'notHelpfulSourcesMissingOrUnreliable', 'notHelpfulOpinionSpeculationOrBias', 'notHelpfulMissingKeyPoints', 'notHelpfulOutdated', 'notHelpfulHardToUnderstand', 'notHelpfulArgumentativeOrBiased', 'notHelpfulOffTopic', 'notHelpfulSpamHarassmentOrAbuse', 'notHelpfulIrrelevantSources', 'notHelpfulOpinionSpeculation', 'notHelpfulNoteNotNeeded', 'ratedOnTweetId']\n",
      "Found user ID column in ratings file: raterParticipantId\n",
      "Original ratings count: 13519068\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3966ae082ff47d3a8f5f8d410f2bee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered ratings count: 8698037\n",
      "Saved filtered ratings to ../data/english_only/english_ratings-00003.tsv\n",
      "Found 98314 unique participants who rated English notes\n"
     ]
    }
   ],
   "source": [
    "# Process ratings file\n",
    "if os.path.exists(ratings_path):\n",
    "    print(f\"Processing ratings file: {ratings_path}\")\n",
    "    # Read the file in chunks to handle large files\n",
    "    chunk_size = 100000\n",
    "    chunks = []\n",
    "    \n",
    "    # Read first chunk to check column names\n",
    "    first_chunk = next(pd.read_csv(ratings_path, sep='\\t', chunksize=1))\n",
    "    print(f\"Ratings file columns: {list(first_chunk.columns)}\")\n",
    "    \n",
    "    # Determine user ID column name\n",
    "    ratings_user_id_column = None\n",
    "    ratings_possible_id_columns = ['participantId', 'raterParticipantId', 'userId', 'ratingParticipantId']\n",
    "    for col in ratings_possible_id_columns:\n",
    "        if col in first_chunk.columns:\n",
    "            ratings_user_id_column = col\n",
    "            print(f\"Found user ID column in ratings file: {ratings_user_id_column}\")\n",
    "            break\n",
    "    \n",
    "    if ratings_user_id_column is None:\n",
    "        print(\"Warning: Could not find user ID column in ratings file, cannot extract raters\")\n",
    "    \n",
    "    # Count total rows in original file\n",
    "    total_ratings_count = 0\n",
    "    for chunk in pd.read_csv(ratings_path, sep='\\t', chunksize=chunk_size):\n",
    "        total_ratings_count += len(chunk)\n",
    "    print(f\"Original ratings count: {total_ratings_count}\")\n",
    "    \n",
    "    # Now filter the data\n",
    "    for chunk in tqdm(pd.read_csv(ratings_path, sep='\\t', chunksize=chunk_size)):\n",
    "        # Filter rows where noteId is in english_note_ids_set\n",
    "        filtered_chunk = chunk[chunk['noteId'].isin(english_note_ids_set)]\n",
    "        chunks.append(filtered_chunk)\n",
    "    \n",
    "    # Combine all filtered chunks\n",
    "    filtered_ratings_df = pd.concat(chunks)\n",
    "    print(f\"Filtered ratings count: {len(filtered_ratings_df)}\")\n",
    "    \n",
    "    # Save filtered ratings\n",
    "    filtered_ratings_df.to_csv(output_ratings_path, sep='\\t', index=False)\n",
    "    print(f\"Saved filtered ratings to {output_ratings_path}\")\n",
    "    \n",
    "    # Extract unique user IDs from ratings\n",
    "    if ratings_user_id_column:\n",
    "        english_raters = set(filtered_ratings_df[ratings_user_id_column].unique())\n",
    "        print(f\"Found {len(english_raters)} unique participants who rated English notes\")\n",
    "    else:\n",
    "        english_raters = set()\n",
    "        print(\"Could not extract rater IDs (column not found)\")\n",
    "else:\n",
    "    print(f\"Ratings file not found: {ratings_path}\")\n",
    "    english_raters = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c2ba5e-2f01-4a14-8584-636159e2f033",
   "metadata": {},
   "source": [
    "# Process note status history file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4040bba5-80e2-49a5-a350-03c7e1e2ee5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing note status history file: ../data/raw/noteStatusHistory-00000.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r1/2mcrr6nx5hg7d13ln1lw2l5w0000gn/T/ipykernel_14742/812526269.py:9: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(note_status_history_path, sep='\\t', chunksize=chunk_size):\n",
      "/var/folders/r1/2mcrr6nx5hg7d13ln1lw2l5w0000gn/T/ipykernel_14742/812526269.py:9: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(note_status_history_path, sep='\\t', chunksize=chunk_size):\n",
      "/var/folders/r1/2mcrr6nx5hg7d13ln1lw2l5w0000gn/T/ipykernel_14742/812526269.py:9: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(note_status_history_path, sep='\\t', chunksize=chunk_size):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original note status history count: 1892439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r1/2mcrr6nx5hg7d13ln1lw2l5w0000gn/T/ipykernel_14742/812526269.py:9: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(note_status_history_path, sep='\\t', chunksize=chunk_size):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "017aa1da1ec84e29b8e8738483086462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages/tqdm/std.py:1181: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for obj in iterable:\n",
      "/Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages/tqdm/std.py:1181: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for obj in iterable:\n",
      "/Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages/tqdm/std.py:1181: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for obj in iterable:\n",
      "/Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages/tqdm/std.py:1181: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for obj in iterable:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered note status history count: 1049809\n",
      "Saved filtered note status history to ../data/english_only/english_noteStatusHistory-00000.tsv\n"
     ]
    }
   ],
   "source": [
    "# Process note status history file\n",
    "if os.path.exists(note_status_history_path):\n",
    "    print(f\"Processing note status history file: {note_status_history_path}\")\n",
    "    chunk_size = 100000\n",
    "    chunks = []\n",
    "    \n",
    "    # Count total rows in original file\n",
    "    total_status_count = 0\n",
    "    for chunk in pd.read_csv(note_status_history_path, sep='\\t', chunksize=chunk_size):\n",
    "        total_status_count += len(chunk)\n",
    "    print(f\"Original note status history count: {total_status_count}\")\n",
    "    \n",
    "    # Now filter the data\n",
    "    for chunk in tqdm(pd.read_csv(note_status_history_path, sep='\\t', chunksize=chunk_size)):\n",
    "        filtered_chunk = chunk[chunk['noteId'].isin(english_note_ids_set)]\n",
    "        chunks.append(filtered_chunk)\n",
    "    \n",
    "    filtered_note_status_history_df = pd.concat(chunks)\n",
    "    print(f\"Filtered note status history count: {len(filtered_note_status_history_df)}\")\n",
    "    \n",
    "    filtered_note_status_history_df.to_csv(output_note_status_history_path, sep='\\t', index=False)\n",
    "    print(f\"Saved filtered note status history to {output_note_status_history_path}\")\n",
    "else:\n",
    "    print(f\"Note status history file not found: {note_status_history_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953eafd2-5e65-4c7b-a863-d4ac0d3ee925",
   "metadata": {},
   "source": [
    "# Process note status history file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8e9357c-73fb-4419-898d-05eee77ea4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing note status history file: ../data/raw/noteStatusHistory-00000.tsv\n",
      "Note status history file columns: ['noteId', 'noteAuthorParticipantId', 'createdAtMillis', 'timestampMillisOfFirstNonNMRStatus', 'firstNonNMRStatus', 'timestampMillisOfCurrentStatus', 'currentStatus', 'timestampMillisOfLatestNonNMRStatus', 'mostRecentNonNMRStatus', 'timestampMillisOfStatusLock', 'lockedStatus', 'timestampMillisOfRetroLock', 'currentCoreStatus', 'currentExpansionStatus', 'currentGroupStatus', 'currentDecidedBy', 'currentModelingGroup', 'timestampMillisOfMostRecentStatusChange', 'timestampMillisOfNmrDueToMinStableCrhTime', 'currentMultiGroupStatus', 'currentModelingMultiGroup', 'timestampMinuteOfFinalScoringOutput', 'timestampMillisOfFirstNmrDueToMinStableCrhTime']\n",
      "Found user ID column in status history file: noteAuthorParticipantId\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r1/2mcrr6nx5hg7d13ln1lw2l5w0000gn/T/ipykernel_14742/2161428688.py:23: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(note_status_history_path, sep='\\t', chunksize=chunk_size):\n",
      "/var/folders/r1/2mcrr6nx5hg7d13ln1lw2l5w0000gn/T/ipykernel_14742/2161428688.py:23: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(note_status_history_path, sep='\\t', chunksize=chunk_size):\n",
      "/var/folders/r1/2mcrr6nx5hg7d13ln1lw2l5w0000gn/T/ipykernel_14742/2161428688.py:23: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(note_status_history_path, sep='\\t', chunksize=chunk_size):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original note status history count: 1892439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r1/2mcrr6nx5hg7d13ln1lw2l5w0000gn/T/ipykernel_14742/2161428688.py:23: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(note_status_history_path, sep='\\t', chunksize=chunk_size):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "764270f82bdb4401bbac72bb105b1381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages/tqdm/std.py:1181: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for obj in iterable:\n",
      "/Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages/tqdm/std.py:1181: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for obj in iterable:\n",
      "/Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages/tqdm/std.py:1181: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for obj in iterable:\n",
      "/Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages/tqdm/std.py:1181: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for obj in iterable:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered note status history count: 1049809\n",
      "Saved filtered note status history to ../data/english_only/english_noteStatusHistory-00000.tsv\n",
      "Total unique English-involved users: 248352\n"
     ]
    }
   ],
   "source": [
    "# Process note status history file\n",
    "if os.path.exists(note_status_history_path):\n",
    "    print(f\"Processing note status history file: {note_status_history_path}\")\n",
    "    \n",
    "    # Read first chunk to check column names\n",
    "    first_status_chunk = next(pd.read_csv(note_status_history_path, sep='\\t', chunksize=1))\n",
    "    print(f\"Note status history file columns: {list(first_status_chunk.columns)}\")\n",
    "    \n",
    "    # Determine user ID column name\n",
    "    status_user_id_column = None\n",
    "    status_possible_id_columns = ['participantId', 'noteAuthorParticipantId', 'authorId']\n",
    "    for col in status_possible_id_columns:\n",
    "        if col in first_status_chunk.columns:\n",
    "            status_user_id_column = col\n",
    "            print(f\"Found user ID column in status history file: {status_user_id_column}\")\n",
    "            break\n",
    "    \n",
    "    chunk_size = 100000\n",
    "    chunks = []\n",
    "    \n",
    "    # Count total rows in original file\n",
    "    total_status_count = 0\n",
    "    for chunk in pd.read_csv(note_status_history_path, sep='\\t', chunksize=chunk_size):\n",
    "        total_status_count += len(chunk)\n",
    "    print(f\"Original note status history count: {total_status_count}\")\n",
    "    \n",
    "    # Now filter the data\n",
    "    for chunk in tqdm(pd.read_csv(note_status_history_path, sep='\\t', chunksize=chunk_size)):\n",
    "        filtered_chunk = chunk[chunk['noteId'].isin(english_note_ids_set)]\n",
    "        chunks.append(filtered_chunk)\n",
    "    \n",
    "    filtered_note_status_history_df = pd.concat(chunks)\n",
    "    print(f\"Filtered note status history count: {len(filtered_note_status_history_df)}\")\n",
    "    \n",
    "    filtered_note_status_history_df.to_csv(output_note_status_history_path, sep='\\t', index=False)\n",
    "    print(f\"Saved filtered note status history to {output_note_status_history_path}\")\n",
    "    \n",
    "    # Extract authors from status history (if not already extracted from notes file)\n",
    "    if status_user_id_column and not english_authors:\n",
    "        status_authors = set(filtered_note_status_history_df[status_user_id_column].unique())\n",
    "        print(f\"Found {len(status_authors)} unique authors of English notes from status history\")\n",
    "        english_authors = status_authors\n",
    "else:\n",
    "    print(f\"Note status history file not found: {note_status_history_path}\")\n",
    "\n",
    "# Merge all English-related user IDs\n",
    "english_users = english_authors.union(english_raters) if english_raters else english_authors\n",
    "print(f\"Total unique English-involved users: {len(english_users)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47032db8-5179-47af-8ea8-443141fc412c",
   "metadata": {},
   "source": [
    "# Process user enrollment file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7322b5af-df17-4ac9-9620-496d3899cbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing user enrollment file: ../data/raw/userEnrollment-00000.tsv\n",
      "User enrollment file columns: ['participantId', 'enrollmentState', 'successfulRatingNeededToEarnIn', 'timestampOfLastStateChange', 'timestampOfLastEarnOut', 'modelingPopulation', 'modelingGroup', 'numberOfTimesEarnedOut']\n",
      "Found user ID column in user enrollment file: participantId\n",
      "Original user enrollment count: 1138360\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7841be964b1e42468ac774f3141c5651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered user enrollment count: 248352\n",
      "Saved filtered user enrollment to ../data/english_only/english_userEnrollment-00000.tsv\n"
     ]
    }
   ],
   "source": [
    "# Process user enrollment file\n",
    "if os.path.exists(user_enrollment_path):\n",
    "    print(f\"Processing user enrollment file: {user_enrollment_path}\")\n",
    "    \n",
    "    # Read first chunk to check column names\n",
    "    first_user_chunk = next(pd.read_csv(user_enrollment_path, sep='\\t', chunksize=1))\n",
    "    print(f\"User enrollment file columns: {list(first_user_chunk.columns)}\")\n",
    "    \n",
    "    # Determine user ID column name\n",
    "    enrollment_user_id_column = None\n",
    "    enrollment_possible_id_columns = ['participantId', 'userId', 'user_id']\n",
    "    for col in enrollment_possible_id_columns:\n",
    "        if col in first_user_chunk.columns:\n",
    "            enrollment_user_id_column = col\n",
    "            print(f\"Found user ID column in user enrollment file: {enrollment_user_id_column}\")\n",
    "            break\n",
    "    \n",
    "    if enrollment_user_id_column is None:\n",
    "        print(\"Warning: Could not find user ID column in enrollment file, cannot filter users\")\n",
    "        # Exit this part of processing if no ID column is found\n",
    "    else:\n",
    "        # Read and filter user enrollment data\n",
    "        chunk_size = 100000\n",
    "        chunks = []\n",
    "        \n",
    "        # Count total rows in original file\n",
    "        total_enrollment_count = 0\n",
    "        for chunk in pd.read_csv(user_enrollment_path, sep='\\t', chunksize=chunk_size):\n",
    "            total_enrollment_count += len(chunk)\n",
    "        print(f\"Original user enrollment count: {total_enrollment_count}\")\n",
    "        \n",
    "        # Now filter the data\n",
    "        for chunk in tqdm(pd.read_csv(user_enrollment_path, sep='\\t', chunksize=chunk_size)):\n",
    "            filtered_chunk = chunk[chunk[enrollment_user_id_column].isin(english_users)]\n",
    "            chunks.append(filtered_chunk)\n",
    "        \n",
    "        filtered_user_enrollment_df = pd.concat(chunks) if chunks else pd.DataFrame()\n",
    "        print(f\"Filtered user enrollment count: {len(filtered_user_enrollment_df)}\")\n",
    "        \n",
    "        filtered_user_enrollment_df.to_csv(output_user_enrollment_path, sep='\\t', index=False)\n",
    "        print(f\"Saved filtered user enrollment to {output_user_enrollment_path}\")\n",
    "else:\n",
    "    print(f\"User enrollment file not found: {user_enrollment_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd3f1493-efa0-48fd-9a9f-637009e98780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All processing complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"All processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb39b337-5b95-4a93-b1dc-a9c643713630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 248352 English user IDs to english_user_ids.npy\n",
      "\n",
      "Summary:\n",
      "Total English notes: 1115550\n",
      "Total unique authors of English notes: 165997\n",
      "Total unique raters of English notes: 98314\n",
      "Total unique English-involved users: 248352\n"
     ]
    }
   ],
   "source": [
    "# Save the set of English-involved user IDs for future reference\n",
    "english_users_list = list(english_users) if 'english_users' in locals() else []\n",
    "np.save(os.path.join(output_dir, \"english_user_ids.npy\"), english_users_list)\n",
    "print(f\"Saved {len(english_users_list)} English user IDs to english_user_ids.npy\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"Total English notes: {len(english_note_ids)}\")\n",
    "if 'english_authors' in locals():\n",
    "    print(f\"Total unique authors of English notes: {len(english_authors)}\")\n",
    "if 'english_raters' in locals():\n",
    "    print(f\"Total unique raters of English notes: {len(english_raters)}\")\n",
    "if 'english_users' in locals():\n",
    "    print(f\"Total unique English-involved users: {len(english_users)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46700266-48a0-46f4-9c0d-b9fbf2f45c6e",
   "metadata": {},
   "source": [
    "## find the cut-off date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "359edb1f-e8cd-4fc7-adef-b6df02d7c8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cut-off Date (timestampMillisOfMostRecentStatusChange): 2025-03-06 01:50:03.146000\n"
     ]
    }
   ],
   "source": [
    "# convert timestampt to dates\n",
    "filtered_note_status_history_df[\"timestampMillisOfMostRecentStatusChange\"] = pd.to_datetime(filtered_note_status_history_df[\"timestampMillisOfMostRecentStatusChange\"], unit=\"ms\")\n",
    "\n",
    "# find the max\n",
    "latest_date = filtered_note_status_history_df[\"timestampMillisOfMostRecentStatusChange\"].max()\n",
    "print(\"Cut-off Date (timestampMillisOfMostRecentStatusChange):\", latest_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df1054e-be68-4d08-a0d4-abc90b12fc9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Twitter Notes)",
   "language": "python",
   "name": "twitter_notes_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
