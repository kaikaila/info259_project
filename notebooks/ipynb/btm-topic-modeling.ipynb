{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BTM Topic Modeling for Twitter Community Notes\n",
    "\n",
    "This notebook implements Biterm Topic Model (BTM) on Twitter Community Notes data. BTM is particularly effective for short texts, making it well-suited for analyzing Twitter-related content.\n",
    "\n",
    "## Why BTM for Twitter Data?\n",
    "- Works well with short texts (tweets, notes)\n",
    "- Handles sparse word co-occurrence patterns\n",
    "- Models word-pair (biterm) occurrences across the corpus\n",
    "- Often outperforms LDA for short text topic discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in /Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages (1.6.1)\n",
      "Requirement already satisfied: matplotlib in /Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages (3.10.1)\n",
      "Requirement already satisfied: seaborn in /Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: wordcloud in /Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages (1.9.4)\n",
      "Requirement already satisfied: btm in /Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages (1.1.5)\n",
      "Requirement already satisfied: tqdm in /Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: nltk in /Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: click in /Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: six>=1.5 in /Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy scikit-learn matplotlib seaborn wordcloud btm tqdm nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yunkaili/spring2025/NLP/project/.venv/bin/python3.10\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/yunkaili/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yunkaili/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/yunkaili/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'btm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# For BTM\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbtm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m oBTM\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Set plot style\u001b[39;00m\n\u001b[1;32m     25\u001b[0m sns\u001b[38;5;241m.\u001b[39mset(style\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhitegrid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'btm'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# For text preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# For BTM\n",
    "from btm import oBTM\n",
    "\n",
    "# Set plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths - update these to match your environment\n",
    "# For Google Colab, you might want to mount Google Drive first\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# SAMPLE_PATH = \"/content/drive/MyDrive/path/to/your/sample_notes-00000.tsv\"\n",
    "\n",
    "# For local use\n",
    "SAMPLE_PATH = os.path.expanduser(\"~/Desktop/samples/sample_notes-00000.tsv\")\n",
    "\n",
    "# Load the sample data\n",
    "df = pd.read_csv(SAMPLE_PATH, sep='\\t')\n",
    "print(f\"Loaded data shape: {df.shape}\")\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'summary' column exists\n",
    "if 'summary' in df.columns:\n",
    "    text_column = 'summary'\n",
    "elif 'noteText' in df.columns:\n",
    "    text_column = 'noteText'\n",
    "else:\n",
    "    # Find potential text columns (columns with string type and longer average length)\n",
    "    text_columns = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            non_null = df[col].dropna()\n",
    "            if len(non_null) > 0:\n",
    "                avg_len = non_null.astype(str).str.len().mean()\n",
    "                if avg_len > 50:  # Assume text fields have avg length > 50\n",
    "                    text_columns.append((col, avg_len))\n",
    "    \n",
    "    text_columns = sorted(text_columns, key=lambda x: x[1], reverse=True)\n",
    "    if text_columns:\n",
    "        text_column = text_columns[0][0]\n",
    "        print(f\"Using '{text_column}' as the text column (avg length: {text_columns[0][1]:.1f})\")\n",
    "    else:\n",
    "        raise ValueError(\"No suitable text column found in the data\")\n",
    "\n",
    "# Show some sample texts\n",
    "print(f\"\\nSample texts from '{text_column}' column:\")\n",
    "for i, text in enumerate(df[text_column].dropna().head(3)):\n",
    "    print(f\"\\nText {i+1}:\\n{text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics of text length\n",
    "text_lengths = df[text_column].dropna().astype(str).str.len()\n",
    "print(f\"Text length statistics:\")\n",
    "print(f\"Mean: {text_lengths.mean():.1f} characters\")\n",
    "print(f\"Median: {text_lengths.median():.1f} characters\")\n",
    "print(f\"Min: {text_lengths.min()} characters\")\n",
    "print(f\"Max: {text_lengths.max()} characters\")\n",
    "\n",
    "# Plot text length distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(text_lengths, bins=50, kde=True)\n",
    "plt.title(f\"Distribution of '{text_column}' Text Lengths\")\n",
    "plt.xlabel(\"Number of Characters\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess text for topic modeling\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    \n",
    "    # Initialize lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs, mentions, hashtags, and special characters\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Filter out very short words\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess all texts\n",
    "print(\"Preprocessing texts...\")\n",
    "docs = df[text_column].dropna().reset_index(drop=True)\n",
    "tokenized_docs = [preprocess_text(doc) for doc in tqdm(docs)]\n",
    "\n",
    "# Filter out empty documents\n",
    "tokenized_docs = [doc for doc in tokenized_docs if len(doc) > 0]\n",
    "print(f\"Retained {len(tokenized_docs)} non-empty documents after preprocessing\")\n",
    "\n",
    "# Show a sample of tokenized documents\n",
    "print(\"\\nSample of preprocessed documents:\")\n",
    "for i, doc in enumerate(tokenized_docs[:3]):\n",
    "    print(f\"Document {i+1}: {' '.join(doc[:20])}{'...' if len(doc) > 20 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary statistics\n",
    "all_words = [word for doc in tokenized_docs for word in doc]\n",
    "unique_words = set(all_words)\n",
    "print(f\"Total words: {len(all_words)}\")\n",
    "print(f\"Unique words: {len(unique_words)}\")\n",
    "\n",
    "# Word frequency\n",
    "from collections import Counter\n",
    "word_freq = Counter(all_words)\n",
    "print(\"\\nTop 20 most frequent words:\")\n",
    "for word, count in word_freq.most_common(20):\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. BTM Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokenized documents to format required by BTM\n",
    "texts_for_btm = [' '.join(doc) for doc in tokenized_docs]\n",
    "\n",
    "# Create vectorizer to get vocabulary\n",
    "vec = CountVectorizer(stop_words='english')\n",
    "X = vec.fit_transform(texts_for_btm)\n",
    "vocabulary = vec.get_feature_names_out()\n",
    "print(f\"Vocabulary size for BTM: {len(vocabulary)}\")\n",
    "\n",
    "# Function to train BTM and evaluate topic coherence\n",
    "def train_evaluate_btm(texts, num_topics, iterations=100):\n",
    "    print(f\"Training BTM with {num_topics} topics...\")\n",
    "    \n",
    "    # Initialize BTM model\n",
    "    btm = oBTM(num_topics=num_topics, V=vocabulary)\n",
    "    \n",
    "    # Fit the model\n",
    "    X_btm = btm.fit_transform(texts, iterations=iterations)\n",
    "    \n",
    "    # Get topic-word distributions\n",
    "    topics = btm.transform_topics()\n",
    "    \n",
    "    return btm, X_btm, topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different numbers of topics\n",
    "topic_counts = [5, 10, 15, 20]\n",
    "models = {}\n",
    "\n",
    "for n_topics in topic_counts:\n",
    "    btm_model, doc_topic_matrix, topic_word_matrix = train_evaluate_btm(texts_for_btm, n_topics, iterations=50)\n",
    "    models[n_topics] = {\n",
    "        'model': btm_model,\n",
    "        'doc_topics': doc_topic_matrix,\n",
    "        'topics': topic_word_matrix\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to display topic results\n",
    "def print_topics(topics, n_words=15):\n",
    "    \"\"\"Print top words for each topic\"\"\"\n",
    "    for i, topic_dist in enumerate(topics):\n",
    "        # Get the top n_words for the topic\n",
    "        top_words_idx = topic_dist.argsort()[-n_words:][::-1]\n",
    "        top_words = [vocabulary[idx] for idx in top_words_idx]\n",
    "        print(f\"Topic #{i+1}: {', '.join(top_words)}\")\n",
    "        print()\n",
    "\n",
    "def plot_wordclouds(topics, n_words=100):\n",
    "    \"\"\"Plot word clouds for each topic\"\"\"\n",
    "    n_topics = len(topics)\n",
    "    n_cols = 2\n",
    "    n_rows = (n_topics + 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "    if n_rows == 1 and n_cols == 1:\n",
    "        axes = np.array([axes])\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, topic_dist in enumerate(topics):\n",
    "        if i < len(axes):\n",
    "            # Create word-weight dictionary for word cloud\n",
    "            word_weights = {}\n",
    "            for word_idx, weight in enumerate(topic_dist):\n",
    "                word = vocabulary[word_idx]\n",
    "                word_weights[word] = weight\n",
    "            \n",
    "            # Generate word cloud\n",
    "            wordcloud = WordCloud(width=800, height=400, \n",
    "                                 background_color='white',\n",
    "                                 max_words=n_words,\n",
    "                                 relative_scaling=0.5,\n",
    "                                 colormap='viridis').generate_from_frequencies(word_weights)\n",
    "            \n",
    "            # Plot\n",
    "            axes[i].imshow(wordcloud, interpolation='bilinear')\n",
    "            axes[i].set_title(f'Topic #{i+1}')\n",
    "            axes[i].axis('off')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for j in range(i+1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Results for Different Topic Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each model, display topics and word clouds\n",
    "for n_topics, model_data in models.items():\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"RESULTS FOR {n_topics} TOPICS\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "    \n",
    "    print(\"Top words for each topic:\")\n",
    "    print_topics(model_data['topics'])\n",
    "    \n",
    "    print(\"Word clouds for each topic:\")\n",
    "    plot_wordclouds(model_data['topics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Choose Best Model and Analyze Document-Topic Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the best model (replace with your chosen number of topics)\n",
    "best_n_topics = 10  # You can change this after reviewing results\n",
    "best_model = models[best_n_topics]\n",
    "\n",
    "# Get document-topic distributions\n",
    "doc_topic_dist = best_model['doc_topics']\n",
    "\n",
    "# Analyze topic prevalence\n",
    "topic_prevalence = doc_topic_dist.mean(axis=0)\n",
    "topic_ids = np.arange(1, best_n_topics + 1)\n",
    "\n",
    "# Plot topic prevalence\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(topic_ids, topic_prevalence, color='skyblue')\n",
    "plt.xlabel('Topic ID')\n",
    "plt.ylabel('Average Topic Probability')\n",
    "plt.title('Topic Prevalence in the Community Notes Corpus')\n",
    "plt.xticks(topic_ids)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "            f'{height:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Examining Documents for Each Topic\n",
    "\n",
    "Now let's look at some example documents that are strongly associated with each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_docs_for_topic(topic_idx, doc_topic_dist, texts, top_n=5):\n",
    "    \"\"\"Get the top documents for a specific topic\"\"\"\n",
    "    # Sort documents by their probability for the given topic\n",
    "    topic_probs = doc_topic_dist[:, topic_idx]\n",
    "    top_doc_indices = topic_probs.argsort()[-top_n:][::-1]\n",
    "    \n",
    "    return [(texts[idx], topic_probs[idx]) for idx in top_doc_indices]\n",
    "\n",
    "# Display example documents for each topic\n",
    "for topic_idx in range(best_n_topics):\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"EXAMPLE DOCUMENTS FOR TOPIC #{topic_idx+1}\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "    \n",
    "    # Get top words for this topic\n",
    "    topic_dist = best_model['topics'][topic_idx]\n",
    "    top_words_idx = topic_dist.argsort()[-10:][::-1]\n",
    "    top_words = [vocabulary[idx] for idx in top_words_idx]\n",
    "    print(f\"Top words: {', '.join(top_words)}\\n\")\n",
    "    \n",
    "    # Get top documents for this topic\n",
    "    top_docs = get_top_docs_for_topic(topic_idx, doc_topic_dist, docs, top_n=3)\n",
    "    \n",
    "    # Display top documents\n",
    "    for i, (doc, prob) in enumerate(top_docs):\n",
    "        print(f\"Document {i+1} (Topic probability: {prob:.4f}):\\n{doc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Manual Topic Interpretation\n",
    "\n",
    "Based on the word distributions and example documents, we can now interpret what each topic represents. This is a manual step that requires human judgment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example interpretation (replace with your own interpretations after analyzing results)\n",
    "topic_interpretations = {\n",
    "    1: \"Topic 1: [Your interpretation here]\",\n",
    "    2: \"Topic 2: [Your interpretation here]\",\n",
    "    3: \"Topic 3: [Your interpretation here]\",\n",
    "    # Add interpretations for all topics...\n",
    "    best_n_topics: f\"Topic {best_n_topics}: [Your interpretation here]\"\n",
    "}\n",
    "\n",
    "# Display interpretations\n",
    "print(\"TOPIC INTERPRETATIONS:\")\n",
    "for topic_id, interpretation in topic_interpretations.items():\n",
    "    print(interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "results_dir = os.path.expanduser(\"~/Desktop/topic_model_results\")\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Save topic-word distributions\n",
    "topic_word_df = pd.DataFrame(best_model['topics'], \n",
    "                             columns=vocabulary)\n",
    "topic_word_df.to_csv(f\"{results_dir}/topic_word_dist_{best_n_topics}_topics.csv\")\n",
    "\n",
    "# Save document-topic distributions\n",
    "doc_topic_df = pd.DataFrame(doc_topic_dist, \n",
    "                           columns=[f\"Topic_{i+1}\" for i in range(best_n_topics)])\n",
    "doc_topic_df.to_csv(f\"{results_dir}/doc_topic_dist_{best_n_topics}_topics.csv\")\n",
    "\n",
    "# Save top words for each topic\n",
    "with open(f\"{results_dir}/top_words_{best_n_topics}_topics.txt\", \"w\") as f:\n",
    "    for i, topic_dist in enumerate(best_model['topics']):\n",
    "        top_words_idx = topic_dist.argsort()[-20:][::-1]\n",
    "        top_words = [vocabulary[idx] for idx in top_words_idx]\n",
    "        f.write(f\"Topic #{i+1}: {', '.join(top_words)}\\n\\n\")\n",
    "\n",
    "print(f\"Results saved to {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "In this notebook, we've applied the Biterm Topic Model (BTM) to Twitter Community Notes data. BTM is particularly well-suited for short text analysis, making it an appropriate choice for social media content. The model identified several coherent topics in the data.\n",
    "\n",
    "Key observations:\n",
    "1. The notes tend to cluster around [number] main themes (based on your interpretation)\n",
    "2. The most prevalent topics appear to be related to [your insights here]\n",
    "3. Examples of clear topics include [your examples here]\n",
    "\n",
    "Next steps:\n",
    "1. Further refine the preprocessing steps if needed\n",
    "2. Consider comparing BTM results with other topic models (LDA, NMF)\n",
    "3. Apply the topic model to the entire dataset (not just the sample)\n",
    "4. Analyze how topics have evolved over time if timestamp data is available"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
