{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ce0dec6-2359-4d5c-94c7-c71fec25fbe8",
   "metadata": {},
   "source": [
    "# Biterm Topic Modeling (BTM) for Community Notes\n",
    "# This script performs topic modeling on English Community Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb100a40-3e55-4a73-a405-608ee4f903f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting biterm\n",
      "  Using cached biterm-0.1.5.tar.gz (79 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[50 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m <string>:2: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         Requirements should be satisfied by a PEP 517 installer.\n",
      "  \u001b[31m   \u001b[0m         If you are using pip, you can try `pip install --use-pep517`.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m /Users/yunkaili/spring2025/NLP/project/.venv/bin/python: No module named pip\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/r1/2mcrr6nx5hg7d13ln1lw2l5w0000gn/T/pip-build-env-m75psrua/overlay/lib/python3.10/site-packages/setuptools/installer.py\", line 107, in _fetch_build_egg_no_warn\n",
      "  \u001b[31m   \u001b[0m     subprocess.check_call(cmd)\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/yunkaili/.pyenv/versions/3.10.12/lib/python3.10/subprocess.py\", line 369, in check_call\n",
      "  \u001b[31m   \u001b[0m     raise CalledProcessError(retcode, cmd)\n",
      "  \u001b[31m   \u001b[0m subprocess.CalledProcessError: Command '['/Users/yunkaili/spring2025/NLP/project/.venv/bin/python', '-m', 'pip', '--disable-pip-version-check', 'wheel', '--no-deps', '-w', '/var/folders/r1/2mcrr6nx5hg7d13ln1lw2l5w0000gn/T/tmpf5ksjwim', '--quiet', 'numpy>=1.10']' returned non-zero exit status 1.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m The above exception was the direct cause of the following exception:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 389, in <module>\n",
      "  \u001b[31m   \u001b[0m     main()\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 373, in main\n",
      "  \u001b[31m   \u001b[0m     json_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 143, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     return hook(config_settings)\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/r1/2mcrr6nx5hg7d13ln1lw2l5w0000gn/T/pip-build-env-m75psrua/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 334, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     return self._get_build_requires(config_settings, requirements=[])\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/r1/2mcrr6nx5hg7d13ln1lw2l5w0000gn/T/pip-build-env-m75psrua/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 304, in _get_build_requires\n",
      "  \u001b[31m   \u001b[0m     self.run_setup()\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/r1/2mcrr6nx5hg7d13ln1lw2l5w0000gn/T/pip-build-env-m75psrua/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 522, in run_setup\n",
      "  \u001b[31m   \u001b[0m     super().run_setup(setup_script=setup_script)\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/r1/2mcrr6nx5hg7d13ln1lw2l5w0000gn/T/pip-build-env-m75psrua/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 320, in run_setup\n",
      "  \u001b[31m   \u001b[0m     exec(code, locals())\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/r1/2mcrr6nx5hg7d13ln1lw2l5w0000gn/T/pip-build-env-m75psrua/overlay/lib/python3.10/site-packages/setuptools/dist.py\", line 663, in fetch_build_eggs\n",
      "  \u001b[31m   \u001b[0m     return _fetch_build_eggs(self, requires)\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/r1/2mcrr6nx5hg7d13ln1lw2l5w0000gn/T/pip-build-env-m75psrua/overlay/lib/python3.10/site-packages/setuptools/installer.py\", line 44, in _fetch_build_eggs\n",
      "  \u001b[31m   \u001b[0m     resolved_dists = pkg_resources.working_set.resolve(\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/r1/2mcrr6nx5hg7d13ln1lw2l5w0000gn/T/pip-build-env-m75psrua/overlay/lib/python3.10/site-packages/pkg_resources/__init__.py\", line 893, in resolve\n",
      "  \u001b[31m   \u001b[0m     dist = self._resolve_dist(\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/r1/2mcrr6nx5hg7d13ln1lw2l5w0000gn/T/pip-build-env-m75psrua/overlay/lib/python3.10/site-packages/pkg_resources/__init__.py\", line 929, in _resolve_dist\n",
      "  \u001b[31m   \u001b[0m     dist = best[req.key] = env.best_match(\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/r1/2mcrr6nx5hg7d13ln1lw2l5w0000gn/T/pip-build-env-m75psrua/overlay/lib/python3.10/site-packages/pkg_resources/__init__.py\", line 1267, in best_match\n",
      "  \u001b[31m   \u001b[0m     return self.obtain(req, installer)\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/r1/2mcrr6nx5hg7d13ln1lw2l5w0000gn/T/pip-build-env-m75psrua/overlay/lib/python3.10/site-packages/pkg_resources/__init__.py\", line 1303, in obtain\n",
      "  \u001b[31m   \u001b[0m     return installer(requirement) if installer else None\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/r1/2mcrr6nx5hg7d13ln1lw2l5w0000gn/T/pip-build-env-m75psrua/overlay/lib/python3.10/site-packages/setuptools/installer.py\", line 109, in _fetch_build_egg_no_warn\n",
      "  \u001b[31m   \u001b[0m     raise DistutilsError(str(e)) from e\n",
      "  \u001b[31m   \u001b[0m distutils.errors.DistutilsError: Command '['/Users/yunkaili/spring2025/NLP/project/.venv/bin/python', '-m', 'pip', '--disable-pip-version-check', 'wheel', '--no-deps', '-w', '/var/folders/r1/2mcrr6nx5hg7d13ln1lw2l5w0000gn/T/tmpf5ksjwim', '--quiet', 'numpy>=1.10']' returned non-zero exit status 1.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install --use-pep517 biterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5cd3c4-caed-42a9-a8a2-59889bdd21bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d65bd52a-9d9b-4b1d-a209-fc89c23e99fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yunkaili/spring2025/NLP/project/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e27f088d-df01-4cf5-91f4-a623226ebb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip 25.0.1 from /Users/yunkaili/spring2025/NLP/project/.venv/lib/python3.10/site-packages/pip (python 3.10)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0eac5b3-e889-475c-a283-f8b1434ad327",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yunkaili/spring2025/NLP/project/.venv/bin/python\n",
      "/Users/yunkaili/spring2025/NLP/project/.venv/bin/pip\n"
     ]
    }
   ],
   "source": [
    "!which python\n",
    "!which pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d791fb1-487a-4b4e-b595-5d3f34847252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Package(s) not found: biterm\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show biterm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92dfb91-cbab-482e-8c63-786680d60621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from biterm.btm import oBTM\n",
    "from biterm.utility import vec_to_biterms\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from wordcloud import WordCloud\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daf31f7-13e7-4d69-b35f-b1ce4f499b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK resources if not already downloaded\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "# Define file paths\n",
    "english_notes_path = \"../english_only/english_notes-00000.tsv\"\n",
    "output_dir = \"../topics/\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load English notes\n",
    "print(\"Loading English notes...\")\n",
    "notes_df = pd.read_csv(english_notes_path, sep='\\t')\n",
    "print(f\"Loaded {len(notes_df)} English notes\")\n",
    "\n",
    "# Display data info\n",
    "print(\"\\nData columns:\")\n",
    "print(notes_df.columns.tolist())\n",
    "\n",
    "# Check if summary column exists\n",
    "if 'summary' not in notes_df.columns:\n",
    "    raise ValueError(\"The 'summary' column is not found in the dataset.\")\n",
    "\n",
    "# Sample data if it's too large\n",
    "max_notes = 50000  # Adjust this based on your computational resources\n",
    "if len(notes_df) > max_notes:\n",
    "    print(f\"\\nSampling {max_notes} notes for topic modeling due to computational constraints...\")\n",
    "    notes_df = notes_df.sample(n=max_notes, random_state=42)\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text) or text.strip() == '':\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs, mentions, and hashtags\n",
    "    text = re.sub(r'http\\S+|@\\S+|#\\S+', '', text)\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Preprocess the text data\n",
    "print(\"\\nPreprocessing text data...\")\n",
    "notes_df['processed_text'] = notes_df['summary'].progress_apply(preprocess_text)\n",
    "\n",
    "# Remove empty texts after preprocessing\n",
    "notes_df = notes_df[notes_df['processed_text'].str.strip() != \"\"]\n",
    "print(f\"After preprocessing, {len(notes_df)} notes remain\")\n",
    "\n",
    "# Save preprocessed data\n",
    "notes_df[['noteId', 'processed_text']].to_csv(os.path.join(output_dir, 'preprocessed_notes.csv'), index=False)\n",
    "\n",
    "# Vectorize the texts\n",
    "print(\"\\nVectorizing the texts...\")\n",
    "vec = CountVectorizer(max_df=0.8, min_df=10)\n",
    "X = vec.fit_transform(notes_df['processed_text']).toarray()\n",
    "vocab = np.array(vec.get_feature_names_out())\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "# Generate biterms\n",
    "print(\"Generating biterms...\")\n",
    "biterms = vec_to_biterms(X)\n",
    "print(f\"Number of biterms: {len(biterms)}\")\n",
    "\n",
    "# Set up BTM model\n",
    "# Tune the number of topics based on your dataset\n",
    "num_topics = 10\n",
    "btm = oBTM(num_topics=num_topics, V=vocab.size)\n",
    "\n",
    "# Train the model\n",
    "print(f\"\\nTraining BTM model with {num_topics} topics...\")\n",
    "btm.fit(biterms, iterations=20)\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Get topics and their top words\n",
    "topics = btm.transform(X)\n",
    "print(\"Topic distribution matrix shape:\", topics.shape)\n",
    "\n",
    "# Save topic model\n",
    "np.save(os.path.join(output_dir, 'topics_matrix.npy'), topics)\n",
    "np.save(os.path.join(output_dir, 'vocabulary.npy'), vocab)\n",
    "\n",
    "# Assign topics to documents\n",
    "notes_df['topic'] = topics.argmax(axis=1)\n",
    "notes_df['topic_probability'] = topics.max(axis=1)\n",
    "\n",
    "# Save documents with assigned topics\n",
    "notes_df[['noteId', 'topic', 'topic_probability']].to_csv(\n",
    "    os.path.join(output_dir, 'notes_with_topics.csv'), index=False)\n",
    "\n",
    "# Generate topic visualization\n",
    "def visualize_topics(btm_model, vocab, num_topics, num_top_words=15):\n",
    "    # Create directory for topic visualizations\n",
    "    topic_vis_dir = os.path.join(output_dir, 'topic_visualizations')\n",
    "    os.makedirs(topic_vis_dir, exist_ok=True)\n",
    "    \n",
    "    # Get topic words distribution\n",
    "    topic_words = btm_model.get_topic_words(num_words=num_top_words)\n",
    "    \n",
    "    # Create a DataFrame to store topic-word distributions\n",
    "    topic_word_df = pd.DataFrame()\n",
    "    \n",
    "    # For each topic, create a visualization\n",
    "    for i in range(num_topics):\n",
    "        # Create word cloud\n",
    "        words = dict(zip(topic_words[i].keys(), topic_words[i].values()))\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=num_top_words)\n",
    "        wordcloud.generate_from_frequencies(words)\n",
    "        \n",
    "        # Plot wordcloud\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Topic {i+1}')\n",
    "        plt.savefig(os.path.join(topic_vis_dir, f'topic_{i+1}_wordcloud.png'), bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Add topic words to DataFrame\n",
    "        topic_word_df[f'Topic_{i+1}'] = pd.Series(topic_words[i])\n",
    "    \n",
    "    # Save topic words to CSV\n",
    "    topic_word_df.to_csv(os.path.join(topic_vis_dir, 'topic_words.csv'))\n",
    "    \n",
    "    # Get documents per topic\n",
    "    topic_counts = notes_df['topic'].value_counts().sort_index()\n",
    "    \n",
    "    # Plot topic distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.barplot(x=topic_counts.index, y=topic_counts.values)\n",
    "    plt.title('Number of Documents per Topic')\n",
    "    plt.xlabel('Topic')\n",
    "    plt.ylabel('Number of Documents')\n",
    "    \n",
    "    # Add count labels on top of bars\n",
    "    for i, count in enumerate(topic_counts.values):\n",
    "        ax.text(i, count + 50, str(count), ha='center')\n",
    "    \n",
    "    plt.savefig(os.path.join(topic_vis_dir, 'topic_distribution.png'), bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return topic_word_df\n",
    "\n",
    "# Generate visualizations\n",
    "print(\"\\nGenerating topic visualizations...\")\n",
    "topic_words_df = visualize_topics(btm, vocab, num_topics)\n",
    "\n",
    "# Create an interactive visualization (optional)\n",
    "try:\n",
    "    print(\"\\nCreating interactive visualization...\")\n",
    "    # Create a CountVectorizer with the same parameters as before\n",
    "    cv = CountVectorizer(max_df=0.8, min_df=10, vocabulary=vec.vocabulary_)\n",
    "    dtm = cv.fit_transform(notes_df['processed_text'])\n",
    "    \n",
    "    # Convert BTM topic-word distribution to format used by pyLDAvis\n",
    "    panel = pyLDAvis.sklearn.prepare(btm, dtm, cv)\n",
    "    pyLDAvis.save_html(panel, os.path.join(output_dir, 'topic_visualization.html'))\n",
    "    print(f\"Interactive visualization saved to {os.path.join(output_dir, 'topic_visualization.html')}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not create interactive visualization: {str(e)}\")\n",
    "\n",
    "# Extract some example notes from each topic\n",
    "def save_topic_examples(df, num_examples=5):\n",
    "    topic_examples_dir = os.path.join(output_dir, 'topic_examples')\n",
    "    os.makedirs(topic_examples_dir, exist_ok=True)\n",
    "    \n",
    "    with open(os.path.join(topic_examples_dir, 'topic_examples.txt'), 'w', encoding='utf-8') as f:\n",
    "        for topic_id in range(num_topics):\n",
    "            f.write(f\"===== TOPIC {topic_id+1} =====\\n\\n\")\n",
    "            \n",
    "            # Get top examples (highest probability)\n",
    "            topic_notes = df[df['topic'] == topic_id].sort_values('topic_probability', ascending=False)\n",
    "            for i, (_, row) in enumerate(topic_notes.head(num_examples).iterrows()):\n",
    "                f.write(f\"Example {i+1} (probability: {row['topic_probability']:.4f}):\\n\")\n",
    "                f.write(f\"Original: {row['summary']}\\n\")\n",
    "                f.write(f\"Processed: {row['processed_text']}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            f.write(\"\\n\\n\")\n",
    "\n",
    "# Save topic examples\n",
    "print(\"\\nSaving example notes for each topic...\")\n",
    "save_topic_examples(notes_df)\n",
    "\n",
    "print(\"\\nTopic modeling complete! Results saved to:\", output_dir)\n",
    "\n",
    "# OPTIONAL: Hyperparameter tuning for number of topics\n",
    "def tune_num_topics(biterms, vocab_size, topic_range, iterations=10):\n",
    "    coherence_scores = []\n",
    "    \n",
    "    for k in topic_range:\n",
    "        print(f\"Testing model with {k} topics...\")\n",
    "        model = oBTM(num_topics=k, V=vocab_size)\n",
    "        model.fit(biterms, iterations=iterations)\n",
    "        \n",
    "        # Calculate coherence (this is just one possible metric)\n",
    "        # In a real implementation, you would need to define a proper coherence metric\n",
    "        coherence = calculate_coherence(model, vocab)\n",
    "        coherence_scores.append(coherence)\n",
    "        \n",
    "        print(f\"Topics: {k}, Coherence: {coherence}\")\n",
    "    \n",
    "    return coherence_scores\n",
    "\n",
    "def calculate_coherence(model, vocabulary):\n",
    "    # This is a placeholder for a real coherence calculation\n",
    "    # In a real implementation, you would use a proper coherence metric like NPMI or UMass\n",
    "    return 0.5  # Placeholder\n",
    "\n",
    "# Uncomment to run hyperparameter tuning\n",
    "\"\"\"\n",
    "print(\"\\nRunning hyperparameter tuning for number of topics...\")\n",
    "topic_range = range(5, 30, 5)\n",
    "coherence_scores = tune_num_topics(biterms, vocab.size, topic_range)\n",
    "\n",
    "# Plot coherence scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(list(topic_range), coherence_scores, marker='o')\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Coherence Score')\n",
    "plt.title('Topic Coherence by Number of Topics')\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(output_dir, 'topic_coherence.png'), bbox_inches='tight')\n",
    "plt.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15e436e-6654-4e86-9ff1-d901574f54c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887e84bb-2c8a-4e90-8713-c1a37713be79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff618eb8-6bac-427a-8555-905e8cf88811",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Twitter Notes)",
   "language": "python",
   "name": "twitter_notes_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
